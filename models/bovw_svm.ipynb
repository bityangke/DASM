{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC \n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iimages into dictionary given a path\n",
    "def load_images(path):\n",
    "    images = {}\n",
    "    for image_category in os.listdir(path):\n",
    "        images_per_category = []\n",
    "        for image in os.listdir(os.path.join(path, image_category)):\n",
    "            image = cv2.imread(os.path.join(path, image_category, image), 0)\n",
    "            if image is not None:\n",
    "                images_per_category.append(image)\n",
    "        images[image_category] = images_per_category\n",
    "        \n",
    "    return images\n",
    "\n",
    "# Feature extractor, supporting SIFT,SURF and ORB features\n",
    "def features_extractor(features_type, images):\n",
    "    descriptors = []\n",
    "    feature_vectors = {}\n",
    "    \n",
    "    if features_type == \"sift\":\n",
    "        feature_extractor = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    elif features_type == \"orb\":\n",
    "        feature_extractor = cv2.ORB_create()\n",
    "    \n",
    "    elif features_type == \"surf\":\n",
    "        feature_extractor = cv2.xfeatures2d.SURF_create()\n",
    "    \n",
    "    for category, value  in images.items():\n",
    "        per_class_features = []\n",
    "        for image in images[category]:\n",
    "            \n",
    "            kp, des = feature_extractor.detectAndCompute(image,None)\n",
    "            descriptors.extend(des)\n",
    "            per_class_features.append(des)\n",
    "        \n",
    "        feature_vectors[category] = per_class_features\n",
    "    \n",
    "    return [descriptors, feature_vectors]  \n",
    "\n",
    "# Kemans clustering over the descriptors\n",
    "def kmeans(k, descriptor_list):\n",
    "    kmeans = KMeans(n_clusters = k, n_init=10)\n",
    "    kmeans.fit(descriptor_list)\n",
    "    visual_words = kmeans.cluster_centers_\n",
    "    return visual_words\n",
    "\n",
    "# Neasrest finding depending on the eculidian distance\n",
    "def find_index(feature, centres):\n",
    "    index = 0\n",
    "    for i in range(len(centres)):\n",
    "        if(i == 0):\n",
    "           count = distance.euclidean(feature, centres[i]) \n",
    "           #count = L1_dist(image, center[i])\n",
    "        else:\n",
    "            dist = distance.euclidean(feature, centres[i]) \n",
    "            #dist = L1_dist(image, center[i])\n",
    "            if(dist < count):\n",
    "                index = i\n",
    "                count = dist\n",
    "    return index    \n",
    "\n",
    "# Build the histograms depending on the Kmeans clusters centres\n",
    "def build_histograms(feature_vectors, visual_words):\n",
    "    per_category_histogram = {}\n",
    "    for key, value in feature_vectors.items():\n",
    "        histograms = []\n",
    "        for image_features in value:\n",
    "            histogram = np.zeros(len(visual_words))\n",
    "            for feature in image_features:\n",
    "                nearest_visual_word_index = find_index(feature, visual_words)\n",
    "                histogram[nearest_visual_word_index] += 1\n",
    "            histograms.append(histogram)\n",
    "        per_category_histogram[key] = histograms\n",
    "    \n",
    "    return per_category_histogram\n",
    "\n",
    "# Histograms matching using KNN\n",
    "def knn(images, tests):\n",
    "    num_test = 0\n",
    "    correct_predict = 0\n",
    "    class_based = {}\n",
    "    preds = []\n",
    "    gts = []\n",
    "    \n",
    "    for test_key, test_val in tests.items():\n",
    "        class_based[test_key] = [0, 0] # [correct, all]\n",
    "        for tst in test_val:\n",
    "            predict_start = 0\n",
    "            #print(test_key)\n",
    "            minimum = 0\n",
    "            key = \"a\" #predicted\n",
    "            for train_key, train_val in images.items():\n",
    "                for train in train_val:\n",
    "                    if(predict_start == 0):\n",
    "                        minimum = distance.euclidean(tst, train)\n",
    "                        #minimum = L1_dist(tst,train)\n",
    "                        key = train_key\n",
    "                        predict_start += 1\n",
    "                    else:\n",
    "                        dist = distance.euclidean(tst, train)\n",
    "                        #dist = L1_dist(tst,train)\n",
    "                        if(dist < minimum):\n",
    "                            minimum = dist\n",
    "                            key = train_key\n",
    "\n",
    "            if(test_key == key):\n",
    "                correct_predict += 1\n",
    "                class_based[test_key][0] += 1\n",
    "                preds.append(key)\n",
    "                gts.append(test_key)\n",
    "            else:\n",
    "                preds.append(key)\n",
    "                gts.append(test_key)\n",
    "    \n",
    "    \n",
    "            num_test += 1\n",
    "            class_based[test_key][1] += 1\n",
    "            #print(minimum)\n",
    "    return [num_test, correct_predict, class_based, preds, gts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images into a dictionary based on category\n",
    "images = load_images('./images')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features extraion is done!\n"
     ]
    }
   ],
   "source": [
    "# Extract features and their descriptors\n",
    "orbs = features_extractor( \"orb\", images)\n",
    "descriptor_list = orbs[0]\n",
    "all_bovw_feature = orbs[1]\n",
    "\n",
    "print(\"features extraction is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans is done!\n"
     ]
    }
   ],
   "source": [
    "# Takes the central points which is visual words\n",
    "visual_words = kmeans(100, descriptor_list)\n",
    "\n",
    "print(\"Kmeans is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building histograms is done!\n"
     ]
    }
   ],
   "source": [
    "# Creates histograms for train data\n",
    "bovw = build_histograms(all_bovw_feature, visual_words)\n",
    "\n",
    "print(\"Building histograms is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the data set\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for key, value in bovw.items():\n",
    "    for histogram in value:\n",
    "        X.append(histogram)\n",
    "        y.append(key)\n",
    "\n",
    "X = np.resize(X, (len(X), len(X[0])))\n",
    "\n",
    "# Normalize the input Array\n",
    "X_normalized = ( X - np.min(X) )  / np.ptp(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d81be6b55d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create a classifier object with the classifier and parameter candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameter_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Train the classifier on data1's feature and target data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# Do grid serach to choose best parameters\n",
    "parameter_candidates = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    "]\n",
    "\n",
    "# Create a classifier object with the classifier and parameter candidates\n",
    "clf = GridSearchCV(estimator=SVC(), param_grid=parameter_candidates, n_jobs = 4, scoring = \"accuracy\")\n",
    "\n",
    "# Train the classifier on data1's feature and target data\n",
    "clf.fit(X_normalized, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.50255577, 0.42731945, 0.77973811, 3.94808412, 0.42931668,\n",
       "        0.53542741, 0.45277309, 0.4645261 , 0.48772446, 0.51452978,\n",
       "        0.53141133, 0.49538064]),\n",
       " 'std_fit_time': array([0.06226764, 0.04773752, 0.09364508, 1.21620459, 0.05196508,\n",
       "        0.09428448, 0.03635393, 0.06994994, 0.05436099, 0.11981623,\n",
       "        0.08112121, 0.05414434]),\n",
       " 'mean_score_time': array([0.15009395, 0.12236587, 0.14226572, 0.15938711, 0.15013854,\n",
       "        0.16285078, 0.14058065, 0.15991712, 0.1661222 , 0.15036178,\n",
       "        0.14657418, 0.12134361]),\n",
       " 'std_score_time': array([0.04114736, 0.00183649, 0.02881952, 0.03718916, 0.02276809,\n",
       "        0.01772472, 0.00560494, 0.01256283, 0.02305543, 0.0215964 ,\n",
       "        0.01601059, 0.01492813]),\n",
       " 'param_C': masked_array(data=[1, 10, 100, 1000, 1, 1, 10, 10, 100, 100, 1000, 1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_kernel': masked_array(data=['linear', 'linear', 'linear', 'linear', 'rbf', 'rbf',\n",
       "                    'rbf', 'rbf', 'rbf', 'rbf', 'rbf', 'rbf'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[--, --, --, --, 0.001, 0.0001, 0.001, 0.0001, 0.001,\n",
       "                    0.0001, 0.001, 0.0001],\n",
       "              mask=[ True,  True,  True,  True, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1, 'kernel': 'linear'},\n",
       "  {'C': 10, 'kernel': 'linear'},\n",
       "  {'C': 100, 'kernel': 'linear'},\n",
       "  {'C': 1000, 'kernel': 'linear'},\n",
       "  {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'},\n",
       "  {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}],\n",
       " 'split0_test_score': array([0.23214286, 0.28968254, 0.27777778, 0.29761905, 0.1765873 ,\n",
       "        0.1765873 , 0.1765873 , 0.1765873 , 0.1765873 , 0.1765873 ,\n",
       "        0.28571429, 0.1765873 ]),\n",
       " 'split1_test_score': array([0.23153693, 0.26746507, 0.25548902, 0.30738523, 0.17764471,\n",
       "        0.17764471, 0.17764471, 0.17764471, 0.17764471, 0.17764471,\n",
       "        0.27744511, 0.17764471]),\n",
       " 'split2_test_score': array([0.2358871 , 0.28024194, 0.29032258, 0.31653226, 0.17741935,\n",
       "        0.17741935, 0.17741935, 0.17741935, 0.17741935, 0.17741935,\n",
       "        0.25806452, 0.17741935]),\n",
       " 'mean_test_score': array([0.23317788, 0.27914724, 0.27448368, 0.30712858, 0.17721519,\n",
       "        0.17721519, 0.17721519, 0.17721519, 0.17721519, 0.17721519,\n",
       "        0.27381746, 0.17721519]),\n",
       " 'std_test_score': array([0.00191935, 0.0091223 , 0.01438641, 0.0077206 , 0.00045577,\n",
       "        0.00045577, 0.00045577, 0.00045577, 0.00045577, 0.00045577,\n",
       "        0.01157231, 0.00045577]),\n",
       " 'rank_test_score': array([5, 2, 3, 1, 6, 6, 6, 6, 6, 6, 4, 6], dtype=int32)}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1000, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30712858094603596"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_index_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
